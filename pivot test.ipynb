{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as f, Window\n",
    "import pandas as pd\n",
    "from kedro.pipeline import *\n",
    "from kedro.io import *\n",
    "from kedro.runner import *\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as f\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session() -> None:\n",
    "    \"\"\"\n",
    "    Placeholder function to create the spark session for a run.\n",
    "\n",
    "    \"\"\"\n",
    "    SparkSession.builder.config(\"spark.driver.memory\", \"16g\").config(\n",
    "        \"spark.executor.memory\", \"16g\"\n",
    "    ).config(\"spark.driver.maxResultSize\", \"8g\").master(\"local[*]\").config(\n",
    "        \"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "    ).config(\n",
    "        \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.1.1,com.crealytics:spark-excel_2.11:0.11.1\"\n",
    "    ).config(\n",
    "        \"fs.s3a.access.key\", \"\"\n",
    "    ).config(\n",
    "        \"fs.s3a.secret.key\", \"\"\n",
    "    ).config(\n",
    "        \"fs.s3a.maxConnections\", \"5000\"\n",
    "    ).config(\n",
    "        \"spark.sql.execution.arrow.enabled\", \"true\"\n",
    "    ).config(\n",
    "        \"spark.debug.maxToStringFields\", \"100\"\n",
    "    ).config(\n",
    "        \"fs.s3a.connection.maximum\", \"5000\"\n",
    "    ).config(\n",
    "        \"spark.sql.shuffle.partitions\", \"8\"\n",
    "    ).config(\n",
    "        \"spark.sql.codegen.wholeStage\", \"false\"\n",
    "    ).appName(\n",
    "        \"comm-analytics\"\n",
    "    ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spark_session()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DateType, StructField, StructType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        [\"1\", \"A\", \"AAA\", 3.1],\n",
    "        [\"1\", \"A\", \"BBB\", 2.4],\n",
    "        [\"1\", \"A\", \"CCC\", 10.34],\n",
    "        [\"2\", \"B\", \"AAA\", 56.45],\n",
    "        [\"2\", \"B\", \"BBB\", 33.44],\n",
    "        [\"2\", \"B\", \"CCC\", 99.23],\n",
    "        [\"3\", \"C\", \"AAA\", 37.56],\n",
    "        [\"3\", \"C\", \"BBB\", 86.89],\n",
    "        [\"3\", \"C\", \"CCC\", 23.89]\n",
    "    ]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"type\", StringType()),\n",
    "        StructField(\"date\", StringType()),\n",
    "        StructField(\"cost\", DoubleType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_data = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-----+\n",
      "| id|type|date| cost|\n",
      "+---+----+----+-----+\n",
      "|  1|   A| AAA|  3.1|\n",
      "|  1|   A| BBB|  2.4|\n",
      "|  1|   A| CCC|10.34|\n",
      "|  2|   B| AAA|56.45|\n",
      "|  2|   B| BBB|33.44|\n",
      "|  2|   B| CCC|99.23|\n",
      "|  3|   C| AAA|37.56|\n",
      "|  3|   C| BBB|86.89|\n",
      "|  3|   C| CCC|23.89|\n",
      "+---+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-----+-----+\n",
      "| id|type|  AAA|  BBB|  CCC|\n",
      "+---+----+-----+-----+-----+\n",
      "|  2|   B|56.45|33.44|99.23|\n",
      "|  1|   A|  3.1|  2.4|10.34|\n",
      "|  3|   C|37.56|86.89|23.89|\n",
      "+---+----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.groupby(df_data.id, df_data.type).pivot(\"date\").avg(\"cost\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "        [\"A\", \"AAA\", 3.1],\n",
    "        [\"A\", \"BBB\", 2.4],\n",
    "        [\"A\", \"CCC\", 10.34],\n",
    "        [\"B\", \"AAA\", 56.45],\n",
    "        [\"B\", \"BBB\", 33.44],\n",
    "        [\"B\", \"CCC\", 99.23],\n",
    "        [\"C\", \"AAA\", 37.56],\n",
    "        [\"C\", \"BBB\", 86.89],\n",
    "        [\"C\", \"CCC\", 23.89]\n",
    "    ]\n",
    "\n",
    "schema1 = StructType(\n",
    "    [        \n",
    "        StructField(\"region_key\", StringType()),\n",
    "        StructField(\"rabatt_status\", StringType()),\n",
    "        StructField(\"vos_pct\", DoubleType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_data1 = spark.createDataFrame(data1, schema=schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+----------------+\n",
      "|region_key|national_vimpat_sales|discounted_sales|\n",
      "+----------+---------------------+----------------+\n",
      "|         B|                56.45|           33.44|\n",
      "|         C|                37.56|           86.89|\n",
      "|         A|                  3.1|             2.4|\n",
      "+----------+---------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data1.groupby(df_data1.region_key).pivot(\"rabatt_status\").avg(\"vos_pct\")\\\n",
    "        .select(\"region_key\", \"AAA\", \"BBB\")\\\n",
    "        .withColumnRenamed(\"AAA\", \"national_vimpat_sales\")\\\n",
    "        .withColumnRenamed(\"BBB\", \"discounted_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
